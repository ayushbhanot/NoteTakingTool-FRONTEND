{"ast":null,"code":"// let mediaRecorder = null; // Declare globally (commented out, we won't use MediaRecorder)\n\n// Function to check WebkitSpeechRecognition support\n/*function checkSpeechRecognitionSupport() {\n    window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!window.SpeechRecognition) {\n        console.error('SpeechRecognition API not supported.');\n        alert('Your browser does not support speech recognition features.');\n        return false;\n    }\n    return true;\n}\n\n// Check for WebkitSpeechRecognition support early\nif (!checkSpeechRecognitionSupport()) {\n    alert('Your browser does not support the necessary speech recognition features.');\n}\n\n// Initialize WebkitSpeechRecognition\nconst recognition = new window.SpeechRecognition();\nrecognition.continuous = true;\nrecognition.interimResults = true; // Set to true to capture partial results\nrecognition.lang = 'en-US';\nlet isRecognitionRunning = false;\nlet finalTranscript = '';  // Store final transcript globally\n\n// Function to request microphone permission\nasync function requestMicPermission() {\n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n        console.log('Microphone permission granted');\n        stream.getTracks().forEach(track => track.stop());  // Stop the stream immediately\n        return true;\n    } catch (error) {\n        console.error('Microphone permission denied:', error);\n        alert('Microphone access is required for speech recognition.');\n        return false;\n    }\n}\n\n// Start speech recognition function\nexport async function startSpeechRecognition() {\n    const permissionGranted = await requestMicPermission();  // Ensure permission is granted\n    if (!permissionGranted) return;\n\n    if (isRecognitionRunning) {\n        console.error(\"Speech recognition is already running.\");\n        return;\n    }\n    try {\n        finalTranscript = '';  // Clear transcript on new session\n        recognition.start();\n        isRecognitionRunning = true;\n        console.log('Speech recognition started.');\n    } catch (error) {\n        console.error('Error starting speech recognition:', error);\n    }\n}\n\n// Stop speech recognition function\nexport function stopSpeechRecognition() {\n    if (!isRecognitionRunning) {\n        console.error(\"Speech recognition is not running.\");\n        return;\n    }\n    console.log('Stopping speech recognition...');\n    recognition.stop();\n}\n\n// Handle when recognition ends\nrecognition.onend = () => {\n    isRecognitionRunning = false;\n    console.log('Speech recognition stopped.');\n    \n    // Send the final transcript to backend for transcription when stopped\n    if (finalTranscript) {\n        processTranscript(finalTranscript);  // Send final transcript to backend\n    } else {\n        console.error(\"No transcript available to send.\");\n    }\n};\n\n// Handle when recognition is aborted\nrecognition.onabort = (event) => {\n    console.error('Speech recognition was aborted:', event);\n    // Optionally, restart recognition if needed\n    setTimeout(() => {\n        console.log('Attempting to restart speech recognition...');\n        startSpeechRecognition();\n    }, 1000);  // Delay to prevent immediate restart\n};\n\n// Process the transcript and send to backend\n// Process the transcript and send to backend\nexport const processTranscript = async (transcript) => {\n    try {\n        // Send as JSON, not FormData, because the backend likely expects a JSON payload\n        const response = await fetch('http://localhost:3001/generateNotes', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ transcript }),  // Send the transcript as JSON\n        });\n\n        if (!response.ok) {\n            throw new Error(`Server error: ${response.status}`);\n        }\n\n        const result = await response.json();\n        console.log('Generated notes:', result);\n    } catch (error) {\n        console.error('Error sending transcript to backend:', error);\n    }\n};\n\n// Event handler for when recognition results are available\nrecognition.onresult = (event) => {\n    let interimTranscript = '';\n\n    for (let i = event.resultIndex; i < event.results.length; i++) {\n        const result = event.results[i];\n        if (result.isFinal) {\n            finalTranscript += result[0].transcript;\n        } else {\n            interimTranscript += result[0].transcript;\n        }\n    }\n\n    console.log('Interim Transcript:', interimTranscript);\n    console.log('Final Transcript:', finalTranscript);  // Only append the final result once\n};\n\nrecognition.onerror = (event) => {\n    console.error('Speech Recognition Error:', event.error);\n\n    // Handle specific \"no-speech\" error\n    if (event.error === 'no-speech') {\n        console.log('No speech detected. Please try speaking more clearly.');\n        alert('No speech detected. Please try again.');\n        // Optionally, restart the recognition\n        setTimeout(() => {\n            console.log('Restarting speech recognition after no-speech error...');\n            startSpeechRecognition();\n        }, 1000);  // Delay before restarting\n    }\n\n    if (event.error === 'audio-capture') {\n        console.error('Microphone access issue.');\n        alert('Please check your microphone permissions.');\n    }\n};\nrecognition.onabort = (event) => {\n    console.error('Speech recognition was aborted:', event);\n    // Optionally restart recognition\n    setTimeout(() => {\n        console.log('Attempting to restart speech recognition...');\n        startSpeechRecognition();\n    }, 1000);  // Delay to prevent immediate restart\n};\n*/\n\n// COMMENTED OUT MEDIARECORDER CODE\n// Declare global variables\n// Declare global variables\n// Declare global variables\n// Declare global variables\nlet mediaRecorder = null;\nlet isRecordingActive = false;\nlet isProcessing = false;\nlet processingQueue = [];\nlet combinedTranscript = \"\";\n\n// Function to start recording\nexport async function startRecording() {\n  if (isRecordingActive) {\n    console.error(\"Recording already in progress.\");\n    return;\n  }\n  console.log('Attempting to start recording...');\n  combinedTranscript = \"\"; // Reset combined transcript on new recording\n  processingQueue = []; // Reset the queue for new recording session\n  isRecordingActive = true; // Mark recording as active\n  isProcessing = false; // Reset isProcessing in case it was left active before\n\n  try {\n    const micStream = await navigator.mediaDevices.getUserMedia({\n      audio: {\n        echoCancellation: false,\n        noiseSuppression: false,\n        sampleRate: 44100\n      }\n    });\n    console.log('Microphone stream acquired:', micStream);\n    const combinedStream = micStream;\n    console.log('Combined stream:', combinedStream);\n    if (combinedStream.getAudioTracks().length === 0) {\n      throw new Error('Combined stream has no audio tracks.');\n    }\n    let options = {\n      mimeType: 'audio/webm; codecs=opus'\n    };\n    if (!MediaRecorder.isTypeSupported(options.mimeType)) {\n      options = {\n        mimeType: 'audio/webm'\n      }; // Fallback to a more general format if needed\n    }\n    mediaRecorder = new MediaRecorder(combinedStream, options);\n    console.log('MediaRecorder initialized:', mediaRecorder);\n    mediaRecorder.ondataavailable = async event => {\n      if (event.data && event.data.size > 10000) {\n        // Only process blobs larger than 10 KB\n        console.log(`Audio chunk available, size: ${event.data.size}`);\n        processingQueue.push(event.data);\n\n        // Start processing the first blob in the queue\n        if (!isProcessing) {\n          console.log('Starting processing of next blob.');\n          await processNextBlob();\n        }\n      } else {\n        console.warn('Received a blob that is too small to process.');\n      }\n    };\n    mediaRecorder.onstart = () => {\n      console.log('Recording started.');\n    };\n    mediaRecorder.onstop = async () => {\n      console.log('Recording stopped.');\n      if (processingQueue.length === 0) {\n        console.log('No more audio chunks to process.');\n      }\n      await processNextBlob(); // Ensure all blobs are processed before stopping\n      console.log('Final Transcript:', combinedTranscript);\n    };\n    mediaRecorder.start(10000); // Chunk every 10 seconds\n    console.log('MediaRecorder started with a 10-second chunk interval.');\n  } catch (error) {\n    console.error('Error starting recording:', error);\n    isRecordingActive = false;\n  }\n}\n\n// Function to process each blob sequentially\nconst processNextBlob = async () => {\n  if (processingQueue.length === 0) {\n    isProcessing = false;\n    return;\n  }\n  isProcessing = true;\n  const blob = processingQueue.shift(); // Get the next blob from the queue\n\n  try {\n    console.log('Processing blob...');\n    const formData = new FormData();\n    formData.append('audio', blob, 'recording.webm');\n    const response = await fetch('http://localhost:3001/transcribe', {\n      method: 'POST',\n      body: formData\n    });\n\n    // Handle non-JSON responses gracefully\n    const resultText = await response.text();\n\n    // Check if the response contains valid JSON\n    try {\n      const resultJson = JSON.parse(resultText);\n      if (resultJson.transcript) {\n        console.log('Transcription result:', resultJson.transcript);\n        combinedTranscript += resultJson.transcript + ' '; // Combine transcripts\n      } else {\n        console.error('No transcript found in the response.');\n      }\n    } catch (error) {\n      console.log('Non-JSON response:', resultText); // Handle non-JSON, such as \"File converted successfully\"\n    }\n  } catch (error) {\n    console.error('Error processing blob or parsing response:', error);\n  } finally {\n    await processNextBlob(); // Continue processing the next blob in the queue\n  }\n};\n\n// Function to stop recording and ensure all blobs are processed\nexport function stopRecording() {\n  if (!isRecordingActive) {\n    console.error(\"Recording is not active.\");\n    return;\n  }\n  console.log('Stopping recording...');\n  mediaRecorder.stop(); // Stop the media recorder\n  isRecordingActive = false; // Mark recording as stopped\n}\n\n// Optional: Function to get the combined transcript\nexport function getTranscript() {\n  return combinedTranscript;\n}\n\n// Function to send the combined transcript to the backend for note generation\nexport async function generateNotesFromTranscript() {\n  if (!combinedTranscript || combinedTranscript.trim() === \"\") {\n    console.error('Transcript is undefined or empty.');\n    return;\n  }\n  console.log('Sending transcript:', combinedTranscript);\n  try {\n    const response = await fetch('http://localhost:3001/generateNotes', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        transcript: combinedTranscript\n      }) // Send combined transcript\n    });\n    if (!response.ok) {\n      throw new Error(`Server error: ${response.status}`);\n    }\n    const result = await response.json();\n    console.log('Generated notes:', result.notes);\n    return result.notes;\n  } catch (error) {\n    console.error('Error generating notes:', error);\n  }\n}\n;","map":{"version":3,"names":["mediaRecorder","isRecordingActive","isProcessing","processingQueue","combinedTranscript","startRecording","console","error","log","micStream","navigator","mediaDevices","getUserMedia","audio","echoCancellation","noiseSuppression","sampleRate","combinedStream","getAudioTracks","length","Error","options","mimeType","MediaRecorder","isTypeSupported","ondataavailable","event","data","size","push","processNextBlob","warn","onstart","onstop","start","blob","shift","formData","FormData","append","response","fetch","method","body","resultText","text","resultJson","JSON","parse","transcript","stopRecording","stop","getTranscript","generateNotesFromTranscript","trim","headers","stringify","ok","status","result","json","notes"],"sources":["/Users/ayushbhanot/Documents/Coding/Riipen/AITranscriptionApp/aitranscriptionapp/src/services/audioRecording.js"],"sourcesContent":["// let mediaRecorder = null; // Declare globally (commented out, we won't use MediaRecorder)\n\n// Function to check WebkitSpeechRecognition support\n/*function checkSpeechRecognitionSupport() {\n    window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!window.SpeechRecognition) {\n        console.error('SpeechRecognition API not supported.');\n        alert('Your browser does not support speech recognition features.');\n        return false;\n    }\n    return true;\n}\n\n// Check for WebkitSpeechRecognition support early\nif (!checkSpeechRecognitionSupport()) {\n    alert('Your browser does not support the necessary speech recognition features.');\n}\n\n// Initialize WebkitSpeechRecognition\nconst recognition = new window.SpeechRecognition();\nrecognition.continuous = true;\nrecognition.interimResults = true; // Set to true to capture partial results\nrecognition.lang = 'en-US';\nlet isRecognitionRunning = false;\nlet finalTranscript = '';  // Store final transcript globally\n\n// Function to request microphone permission\nasync function requestMicPermission() {\n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n        console.log('Microphone permission granted');\n        stream.getTracks().forEach(track => track.stop());  // Stop the stream immediately\n        return true;\n    } catch (error) {\n        console.error('Microphone permission denied:', error);\n        alert('Microphone access is required for speech recognition.');\n        return false;\n    }\n}\n\n// Start speech recognition function\nexport async function startSpeechRecognition() {\n    const permissionGranted = await requestMicPermission();  // Ensure permission is granted\n    if (!permissionGranted) return;\n\n    if (isRecognitionRunning) {\n        console.error(\"Speech recognition is already running.\");\n        return;\n    }\n    try {\n        finalTranscript = '';  // Clear transcript on new session\n        recognition.start();\n        isRecognitionRunning = true;\n        console.log('Speech recognition started.');\n    } catch (error) {\n        console.error('Error starting speech recognition:', error);\n    }\n}\n\n// Stop speech recognition function\nexport function stopSpeechRecognition() {\n    if (!isRecognitionRunning) {\n        console.error(\"Speech recognition is not running.\");\n        return;\n    }\n    console.log('Stopping speech recognition...');\n    recognition.stop();\n}\n\n// Handle when recognition ends\nrecognition.onend = () => {\n    isRecognitionRunning = false;\n    console.log('Speech recognition stopped.');\n    \n    // Send the final transcript to backend for transcription when stopped\n    if (finalTranscript) {\n        processTranscript(finalTranscript);  // Send final transcript to backend\n    } else {\n        console.error(\"No transcript available to send.\");\n    }\n};\n\n// Handle when recognition is aborted\nrecognition.onabort = (event) => {\n    console.error('Speech recognition was aborted:', event);\n    // Optionally, restart recognition if needed\n    setTimeout(() => {\n        console.log('Attempting to restart speech recognition...');\n        startSpeechRecognition();\n    }, 1000);  // Delay to prevent immediate restart\n};\n\n// Process the transcript and send to backend\n// Process the transcript and send to backend\nexport const processTranscript = async (transcript) => {\n    try {\n        // Send as JSON, not FormData, because the backend likely expects a JSON payload\n        const response = await fetch('http://localhost:3001/generateNotes', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ transcript }),  // Send the transcript as JSON\n        });\n\n        if (!response.ok) {\n            throw new Error(`Server error: ${response.status}`);\n        }\n\n        const result = await response.json();\n        console.log('Generated notes:', result);\n    } catch (error) {\n        console.error('Error sending transcript to backend:', error);\n    }\n};\n\n// Event handler for when recognition results are available\nrecognition.onresult = (event) => {\n    let interimTranscript = '';\n\n    for (let i = event.resultIndex; i < event.results.length; i++) {\n        const result = event.results[i];\n        if (result.isFinal) {\n            finalTranscript += result[0].transcript;\n        } else {\n            interimTranscript += result[0].transcript;\n        }\n    }\n\n    console.log('Interim Transcript:', interimTranscript);\n    console.log('Final Transcript:', finalTranscript);  // Only append the final result once\n};\n\nrecognition.onerror = (event) => {\n    console.error('Speech Recognition Error:', event.error);\n\n    // Handle specific \"no-speech\" error\n    if (event.error === 'no-speech') {\n        console.log('No speech detected. Please try speaking more clearly.');\n        alert('No speech detected. Please try again.');\n        // Optionally, restart the recognition\n        setTimeout(() => {\n            console.log('Restarting speech recognition after no-speech error...');\n            startSpeechRecognition();\n        }, 1000);  // Delay before restarting\n    }\n\n    if (event.error === 'audio-capture') {\n        console.error('Microphone access issue.');\n        alert('Please check your microphone permissions.');\n    }\n};\nrecognition.onabort = (event) => {\n    console.error('Speech recognition was aborted:', event);\n    // Optionally restart recognition\n    setTimeout(() => {\n        console.log('Attempting to restart speech recognition...');\n        startSpeechRecognition();\n    }, 1000);  // Delay to prevent immediate restart\n};\n*/\n\n// COMMENTED OUT MEDIARECORDER CODE\n// Declare global variables\n// Declare global variables\n// Declare global variables\n// Declare global variables\nlet mediaRecorder = null;\nlet isRecordingActive = false;\nlet isProcessing = false;\nlet processingQueue = [];\nlet combinedTranscript = \"\";\n\n// Function to start recording\nexport async function startRecording() {\n    if (isRecordingActive) {\n        console.error(\"Recording already in progress.\");\n        return;\n    }\n\n    console.log('Attempting to start recording...');\n    combinedTranscript = \"\";  // Reset combined transcript on new recording\n    processingQueue = [];  // Reset the queue for new recording session\n    isRecordingActive = true; // Mark recording as active\n    isProcessing = false; // Reset isProcessing in case it was left active before\n\n    try {\n        const micStream = await navigator.mediaDevices.getUserMedia({\n            audio: {\n                echoCancellation: false,\n                noiseSuppression: false,\n                sampleRate: 44100\n            }\n        });\n\n        console.log('Microphone stream acquired:', micStream);\n\n        const combinedStream = micStream;\n        console.log('Combined stream:', combinedStream);\n\n        if (combinedStream.getAudioTracks().length === 0) {\n            throw new Error('Combined stream has no audio tracks.');\n        }\n\n        let options = { mimeType: 'audio/webm; codecs=opus' };\n        if (!MediaRecorder.isTypeSupported(options.mimeType)) {\n            options = { mimeType: 'audio/webm' };  // Fallback to a more general format if needed\n        }\n\n        mediaRecorder = new MediaRecorder(combinedStream, options);\n        console.log('MediaRecorder initialized:', mediaRecorder);\n\n        mediaRecorder.ondataavailable = async (event) => {\n            if (event.data && event.data.size > 10000) { // Only process blobs larger than 10 KB\n                console.log(`Audio chunk available, size: ${event.data.size}`);\n                processingQueue.push(event.data);\n\n                // Start processing the first blob in the queue\n                if (!isProcessing) {\n                    console.log('Starting processing of next blob.');\n                    await processNextBlob();\n                }\n            } else {\n                console.warn('Received a blob that is too small to process.');\n            }\n        };\n\n        mediaRecorder.onstart = () => {\n            console.log('Recording started.');\n        };\n\n        mediaRecorder.onstop = async () => {\n            console.log('Recording stopped.');\n            if (processingQueue.length === 0) {\n                console.log('No more audio chunks to process.');\n            }\n            await processNextBlob(); // Ensure all blobs are processed before stopping\n            console.log('Final Transcript:', combinedTranscript);\n        };\n\n        mediaRecorder.start(10000); // Chunk every 10 seconds\n        console.log('MediaRecorder started with a 10-second chunk interval.');\n\n    } catch (error) {\n        console.error('Error starting recording:', error);\n        isRecordingActive = false;\n    }\n}\n\n// Function to process each blob sequentially\nconst processNextBlob = async () => {\n    if (processingQueue.length === 0) {\n        isProcessing = false;\n        return;\n    }\n\n    isProcessing = true;\n    const blob = processingQueue.shift();  // Get the next blob from the queue\n\n    try {\n        console.log('Processing blob...');\n        const formData = new FormData();\n        formData.append('audio', blob, 'recording.webm');\n\n        const response = await fetch('http://localhost:3001/transcribe', {\n            method: 'POST',\n            body: formData,\n        });\n\n        // Handle non-JSON responses gracefully\n        const resultText = await response.text();\n\n        // Check if the response contains valid JSON\n        try {\n            const resultJson = JSON.parse(resultText);\n            if (resultJson.transcript) {\n                console.log('Transcription result:', resultJson.transcript);\n                combinedTranscript += resultJson.transcript + ' '; // Combine transcripts\n            } else {\n                console.error('No transcript found in the response.');\n            }\n        } catch (error) {\n            console.log('Non-JSON response:', resultText); // Handle non-JSON, such as \"File converted successfully\"\n        }\n\n    } catch (error) {\n        console.error('Error processing blob or parsing response:', error);\n    } finally {\n        await processNextBlob();  // Continue processing the next blob in the queue\n    }\n};\n\n// Function to stop recording and ensure all blobs are processed\nexport function stopRecording() {\n    if (!isRecordingActive) {\n        console.error(\"Recording is not active.\");\n        return;\n    }\n\n    console.log('Stopping recording...');\n    mediaRecorder.stop();  // Stop the media recorder\n    isRecordingActive = false; // Mark recording as stopped\n}\n\n// Optional: Function to get the combined transcript\nexport function getTranscript() {\n    return combinedTranscript;\n}\n\n// Function to send the combined transcript to the backend for note generation\nexport async function generateNotesFromTranscript() {\n    if (!combinedTranscript || combinedTranscript.trim() === \"\") {\n        console.error('Transcript is undefined or empty.');\n        return;\n    }\n\n    console.log('Sending transcript:', combinedTranscript);\n\n    try {\n        const response = await fetch('http://localhost:3001/generateNotes', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ transcript: combinedTranscript }),  // Send combined transcript\n        });\n\n        if (!response.ok) {\n            throw new Error(`Server error: ${response.status}`);\n        }\n\n        const result = await response.json();\n        console.log('Generated notes:', result.notes);\n        return result.notes;\n    } catch (error) {\n        console.error('Error generating notes:', error);\n    }\n};\n"],"mappings":"AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAIA,aAAa,GAAG,IAAI;AACxB,IAAIC,iBAAiB,GAAG,KAAK;AAC7B,IAAIC,YAAY,GAAG,KAAK;AACxB,IAAIC,eAAe,GAAG,EAAE;AACxB,IAAIC,kBAAkB,GAAG,EAAE;;AAE3B;AACA,OAAO,eAAeC,cAAcA,CAAA,EAAG;EACnC,IAAIJ,iBAAiB,EAAE;IACnBK,OAAO,CAACC,KAAK,CAAC,gCAAgC,CAAC;IAC/C;EACJ;EAEAD,OAAO,CAACE,GAAG,CAAC,kCAAkC,CAAC;EAC/CJ,kBAAkB,GAAG,EAAE,CAAC,CAAE;EAC1BD,eAAe,GAAG,EAAE,CAAC,CAAE;EACvBF,iBAAiB,GAAG,IAAI,CAAC,CAAC;EAC1BC,YAAY,GAAG,KAAK,CAAC,CAAC;;EAEtB,IAAI;IACA,MAAMO,SAAS,GAAG,MAAMC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;MACxDC,KAAK,EAAE;QACHC,gBAAgB,EAAE,KAAK;QACvBC,gBAAgB,EAAE,KAAK;QACvBC,UAAU,EAAE;MAChB;IACJ,CAAC,CAAC;IAEFV,OAAO,CAACE,GAAG,CAAC,6BAA6B,EAAEC,SAAS,CAAC;IAErD,MAAMQ,cAAc,GAAGR,SAAS;IAChCH,OAAO,CAACE,GAAG,CAAC,kBAAkB,EAAES,cAAc,CAAC;IAE/C,IAAIA,cAAc,CAACC,cAAc,CAAC,CAAC,CAACC,MAAM,KAAK,CAAC,EAAE;MAC9C,MAAM,IAAIC,KAAK,CAAC,sCAAsC,CAAC;IAC3D;IAEA,IAAIC,OAAO,GAAG;MAAEC,QAAQ,EAAE;IAA0B,CAAC;IACrD,IAAI,CAACC,aAAa,CAACC,eAAe,CAACH,OAAO,CAACC,QAAQ,CAAC,EAAE;MAClDD,OAAO,GAAG;QAAEC,QAAQ,EAAE;MAAa,CAAC,CAAC,CAAE;IAC3C;IAEAtB,aAAa,GAAG,IAAIuB,aAAa,CAACN,cAAc,EAAEI,OAAO,CAAC;IAC1Df,OAAO,CAACE,GAAG,CAAC,4BAA4B,EAAER,aAAa,CAAC;IAExDA,aAAa,CAACyB,eAAe,GAAG,MAAOC,KAAK,IAAK;MAC7C,IAAIA,KAAK,CAACC,IAAI,IAAID,KAAK,CAACC,IAAI,CAACC,IAAI,GAAG,KAAK,EAAE;QAAE;QACzCtB,OAAO,CAACE,GAAG,CAAC,gCAAgCkB,KAAK,CAACC,IAAI,CAACC,IAAI,EAAE,CAAC;QAC9DzB,eAAe,CAAC0B,IAAI,CAACH,KAAK,CAACC,IAAI,CAAC;;QAEhC;QACA,IAAI,CAACzB,YAAY,EAAE;UACfI,OAAO,CAACE,GAAG,CAAC,mCAAmC,CAAC;UAChD,MAAMsB,eAAe,CAAC,CAAC;QAC3B;MACJ,CAAC,MAAM;QACHxB,OAAO,CAACyB,IAAI,CAAC,+CAA+C,CAAC;MACjE;IACJ,CAAC;IAED/B,aAAa,CAACgC,OAAO,GAAG,MAAM;MAC1B1B,OAAO,CAACE,GAAG,CAAC,oBAAoB,CAAC;IACrC,CAAC;IAEDR,aAAa,CAACiC,MAAM,GAAG,YAAY;MAC/B3B,OAAO,CAACE,GAAG,CAAC,oBAAoB,CAAC;MACjC,IAAIL,eAAe,CAACgB,MAAM,KAAK,CAAC,EAAE;QAC9Bb,OAAO,CAACE,GAAG,CAAC,kCAAkC,CAAC;MACnD;MACA,MAAMsB,eAAe,CAAC,CAAC,CAAC,CAAC;MACzBxB,OAAO,CAACE,GAAG,CAAC,mBAAmB,EAAEJ,kBAAkB,CAAC;IACxD,CAAC;IAEDJ,aAAa,CAACkC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC;IAC5B5B,OAAO,CAACE,GAAG,CAAC,wDAAwD,CAAC;EAEzE,CAAC,CAAC,OAAOD,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;IACjDN,iBAAiB,GAAG,KAAK;EAC7B;AACJ;;AAEA;AACA,MAAM6B,eAAe,GAAG,MAAAA,CAAA,KAAY;EAChC,IAAI3B,eAAe,CAACgB,MAAM,KAAK,CAAC,EAAE;IAC9BjB,YAAY,GAAG,KAAK;IACpB;EACJ;EAEAA,YAAY,GAAG,IAAI;EACnB,MAAMiC,IAAI,GAAGhC,eAAe,CAACiC,KAAK,CAAC,CAAC,CAAC,CAAE;;EAEvC,IAAI;IACA9B,OAAO,CAACE,GAAG,CAAC,oBAAoB,CAAC;IACjC,MAAM6B,QAAQ,GAAG,IAAIC,QAAQ,CAAC,CAAC;IAC/BD,QAAQ,CAACE,MAAM,CAAC,OAAO,EAAEJ,IAAI,EAAE,gBAAgB,CAAC;IAEhD,MAAMK,QAAQ,GAAG,MAAMC,KAAK,CAAC,kCAAkC,EAAE;MAC7DC,MAAM,EAAE,MAAM;MACdC,IAAI,EAAEN;IACV,CAAC,CAAC;;IAEF;IACA,MAAMO,UAAU,GAAG,MAAMJ,QAAQ,CAACK,IAAI,CAAC,CAAC;;IAExC;IACA,IAAI;MACA,MAAMC,UAAU,GAAGC,IAAI,CAACC,KAAK,CAACJ,UAAU,CAAC;MACzC,IAAIE,UAAU,CAACG,UAAU,EAAE;QACvB3C,OAAO,CAACE,GAAG,CAAC,uBAAuB,EAAEsC,UAAU,CAACG,UAAU,CAAC;QAC3D7C,kBAAkB,IAAI0C,UAAU,CAACG,UAAU,GAAG,GAAG,CAAC,CAAC;MACvD,CAAC,MAAM;QACH3C,OAAO,CAACC,KAAK,CAAC,sCAAsC,CAAC;MACzD;IACJ,CAAC,CAAC,OAAOA,KAAK,EAAE;MACZD,OAAO,CAACE,GAAG,CAAC,oBAAoB,EAAEoC,UAAU,CAAC,CAAC,CAAC;IACnD;EAEJ,CAAC,CAAC,OAAOrC,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,4CAA4C,EAAEA,KAAK,CAAC;EACtE,CAAC,SAAS;IACN,MAAMuB,eAAe,CAAC,CAAC,CAAC,CAAE;EAC9B;AACJ,CAAC;;AAED;AACA,OAAO,SAASoB,aAAaA,CAAA,EAAG;EAC5B,IAAI,CAACjD,iBAAiB,EAAE;IACpBK,OAAO,CAACC,KAAK,CAAC,0BAA0B,CAAC;IACzC;EACJ;EAEAD,OAAO,CAACE,GAAG,CAAC,uBAAuB,CAAC;EACpCR,aAAa,CAACmD,IAAI,CAAC,CAAC,CAAC,CAAE;EACvBlD,iBAAiB,GAAG,KAAK,CAAC,CAAC;AAC/B;;AAEA;AACA,OAAO,SAASmD,aAAaA,CAAA,EAAG;EAC5B,OAAOhD,kBAAkB;AAC7B;;AAEA;AACA,OAAO,eAAeiD,2BAA2BA,CAAA,EAAG;EAChD,IAAI,CAACjD,kBAAkB,IAAIA,kBAAkB,CAACkD,IAAI,CAAC,CAAC,KAAK,EAAE,EAAE;IACzDhD,OAAO,CAACC,KAAK,CAAC,mCAAmC,CAAC;IAClD;EACJ;EAEAD,OAAO,CAACE,GAAG,CAAC,qBAAqB,EAAEJ,kBAAkB,CAAC;EAEtD,IAAI;IACA,MAAMoC,QAAQ,GAAG,MAAMC,KAAK,CAAC,qCAAqC,EAAE;MAChEC,MAAM,EAAE,MAAM;MACda,OAAO,EAAE;QACL,cAAc,EAAE;MACpB,CAAC;MACDZ,IAAI,EAAEI,IAAI,CAACS,SAAS,CAAC;QAAEP,UAAU,EAAE7C;MAAmB,CAAC,CAAC,CAAG;IAC/D,CAAC,CAAC;IAEF,IAAI,CAACoC,QAAQ,CAACiB,EAAE,EAAE;MACd,MAAM,IAAIrC,KAAK,CAAC,iBAAiBoB,QAAQ,CAACkB,MAAM,EAAE,CAAC;IACvD;IAEA,MAAMC,MAAM,GAAG,MAAMnB,QAAQ,CAACoB,IAAI,CAAC,CAAC;IACpCtD,OAAO,CAACE,GAAG,CAAC,kBAAkB,EAAEmD,MAAM,CAACE,KAAK,CAAC;IAC7C,OAAOF,MAAM,CAACE,KAAK;EACvB,CAAC,CAAC,OAAOtD,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,yBAAyB,EAAEA,KAAK,CAAC;EACnD;AACJ;AAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}