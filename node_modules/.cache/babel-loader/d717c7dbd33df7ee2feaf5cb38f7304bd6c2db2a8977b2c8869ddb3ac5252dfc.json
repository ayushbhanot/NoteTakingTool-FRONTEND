{"ast":null,"code":"let isRecognitionRunning = false;\nlet finalTranscript = '';\nlet recognition;\n\n// Function to check WebkitSpeechRecognition support\nfunction checkSpeechRecognitionSupport() {\n  window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n  if (!window.SpeechRecognition) {\n    console.error('SpeechRecognition API not supported.');\n    alert('Your browser does not support speech recognition features.');\n    return false;\n  }\n  return true;\n}\n\n// Check for WebkitSpeechRecognition support early\nif (!checkSpeechRecognitionSupport()) {\n  alert('Your browser does not support the necessary speech recognition features.');\n}\n\n// Initialize WebkitSpeechRecognition\nfunction initializeRecognition() {\n  recognition = new window.SpeechRecognition();\n  recognition.continuous = true; // Allow continuous recognition\n  recognition.interimResults = true; // Capture partial results for real-time updates\n  recognition.lang = 'en-US'; // Set recognition language\n\n  // Handle recognition results (interim and final)\n  recognition.onresult = event => {\n    let interimTranscript = '';\n\n    // Iterate through results starting from the event.resultIndex\n    for (let i = event.resultIndex; i < event.results.length; i++) {\n      const result = event.results[i];\n\n      // If the result is final, append it to the final transcript\n      if (result.isFinal) {\n        finalTranscript += result[0].transcript; // Append to final transcript\n        console.log('Final Transcript So Far:', finalTranscript); // Log final transcript\n      } else {\n        interimTranscript += result[0].transcript; // Append interim transcripts\n      }\n    }\n\n    // Log interim transcript (optional)\n    console.log('Interim Transcript:', interimTranscript);\n  };\n\n  // Handle when recognition ends, and restart if necessary\n  recognition.onend = () => {\n    console.log('Speech recognition stopped.');\n\n    // If there's a final transcript, send it to the backend for processing\n    if (finalTranscript) {\n      console.log('Sending final transcript to backend.');\n      processTranscript(finalTranscript); // Send the final transcript to the backend\n      finalTranscript = ''; // Clear the final transcript for the next session\n    }\n\n    // If the user didn't stop it manually, restart recognition\n    if (isRecognitionRunning) {\n      console.log('Restarting speech recognition...');\n      recognition.start(); // Restart recognition automatically\n    }\n  };\n\n  // Handle errors during recognition\n  recognition.onerror = event => {\n    console.error('Speech Recognition Error:', event.error);\n\n    // Handle no-speech error by restarting recognition\n    if (event.error === 'no-speech') {\n      alert('No speech detected. Please try again.');\n      setTimeout(() => {\n        startSpeechRecognition();\n      }, 1000); // Restart after 1 second if no speech is detected\n    }\n\n    // Handle microphone permission errors\n    if (event.error === 'audio-capture') {\n      alert('Please check your microphone permissions.');\n    }\n  };\n}\n\n// Function to request microphone permission\nasync function requestMicPermission() {\n  try {\n    const stream = await navigator.mediaDevices.getUserMedia({\n      audio: true\n    });\n    console.log('Microphone permission granted');\n    stream.getTracks().forEach(track => track.stop()); // Stop the stream immediately after permission is granted\n    return true;\n  } catch (error) {\n    console.error('Microphone permission denied:', error);\n    alert('Microphone access is required for speech recognition.');\n    return false;\n  }\n}\n\n// Start speech recognition function\nexport async function startSpeechRecognition() {\n  const permissionGranted = await requestMicPermission(); // Ensure microphone permission is granted\n  if (!permissionGranted) return;\n  if (isRecognitionRunning) {\n    console.error(\"Speech recognition is already running.\");\n    return;\n  }\n  finalTranscript = ''; // Clear the transcript at the start of a new session\n  initializeRecognition(); // Initialize the recognition instance\n  try {\n    recognition.start(); // Start speech recognition\n    isRecognitionRunning = true; // Mark recognition as running\n    console.log('Speech recognition started.');\n  } catch (error) {\n    console.error('Error starting speech recognition:', error);\n  }\n}\n\n// Stop speech recognition function\nexport function stopSpeechRecognition() {\n  if (!isRecognitionRunning) {\n    console.error(\"Speech recognition is not running.\");\n    return;\n  }\n  console.log('Stopping speech recognition...');\n  recognition.stop(); // Stop recognition\n  isRecognitionRunning = false; // Mark recognition as stopped\n}\n\n// Process the final transcript and send to backend\nexport const processTranscript = async transcript => {\n  try {\n    // Send as JSON because the backend likely expects a JSON payload\n    const response = await fetch('http://localhost:3001/generateNotes', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        transcript\n      }) // Send the transcript as JSON\n    });\n    if (!response.ok) {\n      throw new Error(`Server error: ${response.status}`);\n    }\n    const result = await response.json();\n    console.log('Generated notes:', result); // Log the generated notes from the backend\n  } catch (error) {\n    console.error('Error sending transcript to backend:', error); // Log errors that occur during the API call\n  }\n};","map":{"version":3,"names":["isRecognitionRunning","finalTranscript","recognition","checkSpeechRecognitionSupport","window","SpeechRecognition","webkitSpeechRecognition","console","error","alert","initializeRecognition","continuous","interimResults","lang","onresult","event","interimTranscript","i","resultIndex","results","length","result","isFinal","transcript","log","onend","processTranscript","start","onerror","setTimeout","startSpeechRecognition","requestMicPermission","stream","navigator","mediaDevices","getUserMedia","audio","getTracks","forEach","track","stop","permissionGranted","stopSpeechRecognition","response","fetch","method","headers","body","JSON","stringify","ok","Error","status","json"],"sources":["/Users/ayushbhanot/Documents/Coding/Riipen/AITranscriptionApp/aitranscriptionapp/src/services/audioRecording.js"],"sourcesContent":["let isRecognitionRunning = false;\nlet finalTranscript = ''; \nlet recognition;\n\n// Function to check WebkitSpeechRecognition support\nfunction checkSpeechRecognitionSupport() {\n    window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!window.SpeechRecognition) {\n        console.error('SpeechRecognition API not supported.');\n        alert('Your browser does not support speech recognition features.');\n        return false;\n    }\n    return true;\n}\n\n// Check for WebkitSpeechRecognition support early\nif (!checkSpeechRecognitionSupport()) {\n    alert('Your browser does not support the necessary speech recognition features.');\n}\n\n// Initialize WebkitSpeechRecognition\nfunction initializeRecognition() {\n    recognition = new window.SpeechRecognition();\n    recognition.continuous = true;  // Allow continuous recognition\n    recognition.interimResults = true;  // Capture partial results for real-time updates\n    recognition.lang = 'en-US';  // Set recognition language\n\n    // Handle recognition results (interim and final)\n    recognition.onresult = (event) => {\n        let interimTranscript = '';\n\n        // Iterate through results starting from the event.resultIndex\n        for (let i = event.resultIndex; i < event.results.length; i++) {\n            const result = event.results[i];\n\n            // If the result is final, append it to the final transcript\n            if (result.isFinal) {\n                finalTranscript += result[0].transcript;  // Append to final transcript\n                console.log('Final Transcript So Far:', finalTranscript);  // Log final transcript\n            } else {\n                interimTranscript += result[0].transcript;  // Append interim transcripts\n            }\n        }\n\n        // Log interim transcript (optional)\n        console.log('Interim Transcript:', interimTranscript);\n    };\n\n    // Handle when recognition ends, and restart if necessary\n    recognition.onend = () => {\n        console.log('Speech recognition stopped.');\n\n        // If there's a final transcript, send it to the backend for processing\n        if (finalTranscript) {\n            console.log('Sending final transcript to backend.');\n            processTranscript(finalTranscript);  // Send the final transcript to the backend\n            finalTranscript = '';  // Clear the final transcript for the next session\n        }\n\n        // If the user didn't stop it manually, restart recognition\n        if (isRecognitionRunning) {\n            console.log('Restarting speech recognition...');\n            recognition.start();  // Restart recognition automatically\n        }\n    };\n\n    // Handle errors during recognition\n    recognition.onerror = (event) => {\n        console.error('Speech Recognition Error:', event.error);\n\n        // Handle no-speech error by restarting recognition\n        if (event.error === 'no-speech') {\n            alert('No speech detected. Please try again.');\n            setTimeout(() => {\n                startSpeechRecognition();\n            }, 1000);  // Restart after 1 second if no speech is detected\n        }\n\n        // Handle microphone permission errors\n        if (event.error === 'audio-capture') {\n            alert('Please check your microphone permissions.');\n        }\n    };\n}\n\n// Function to request microphone permission\nasync function requestMicPermission() {\n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n        console.log('Microphone permission granted');\n        stream.getTracks().forEach(track => track.stop());  // Stop the stream immediately after permission is granted\n        return true;\n    } catch (error) {\n        console.error('Microphone permission denied:', error);\n        alert('Microphone access is required for speech recognition.');\n        return false;\n    }\n}\n\n// Start speech recognition function\nexport async function startSpeechRecognition() {\n    const permissionGranted = await requestMicPermission();  // Ensure microphone permission is granted\n    if (!permissionGranted) return;\n\n    if (isRecognitionRunning) {\n        console.error(\"Speech recognition is already running.\");\n        return;\n    }\n\n    finalTranscript = '';  // Clear the transcript at the start of a new session\n    initializeRecognition();  // Initialize the recognition instance\n    try {\n        recognition.start();  // Start speech recognition\n        isRecognitionRunning = true;  // Mark recognition as running\n        console.log('Speech recognition started.');\n    } catch (error) {\n        console.error('Error starting speech recognition:', error);\n    }\n}\n\n// Stop speech recognition function\nexport function stopSpeechRecognition() {\n    if (!isRecognitionRunning) {\n        console.error(\"Speech recognition is not running.\");\n        return;\n    }\n    console.log('Stopping speech recognition...');\n    recognition.stop();  // Stop recognition\n    isRecognitionRunning = false;  // Mark recognition as stopped\n}\n\n// Process the final transcript and send to backend\nexport const processTranscript = async (transcript) => {\n    try {\n        // Send as JSON because the backend likely expects a JSON payload\n        const response = await fetch('http://localhost:3001/generateNotes', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ transcript }),  // Send the transcript as JSON\n        });\n\n        if (!response.ok) {\n            throw new Error(`Server error: ${response.status}`);\n        }\n\n        const result = await response.json();\n        console.log('Generated notes:', result);  // Log the generated notes from the backend\n    } catch (error) {\n        console.error('Error sending transcript to backend:', error);  // Log errors that occur during the API call\n    }\n};\n"],"mappings":"AAAA,IAAIA,oBAAoB,GAAG,KAAK;AAChC,IAAIC,eAAe,GAAG,EAAE;AACxB,IAAIC,WAAW;;AAEf;AACA,SAASC,6BAA6BA,CAAA,EAAG;EACrCC,MAAM,CAACC,iBAAiB,GAAGD,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;EACrF,IAAI,CAACF,MAAM,CAACC,iBAAiB,EAAE;IAC3BE,OAAO,CAACC,KAAK,CAAC,sCAAsC,CAAC;IACrDC,KAAK,CAAC,4DAA4D,CAAC;IACnE,OAAO,KAAK;EAChB;EACA,OAAO,IAAI;AACf;;AAEA;AACA,IAAI,CAACN,6BAA6B,CAAC,CAAC,EAAE;EAClCM,KAAK,CAAC,0EAA0E,CAAC;AACrF;;AAEA;AACA,SAASC,qBAAqBA,CAAA,EAAG;EAC7BR,WAAW,GAAG,IAAIE,MAAM,CAACC,iBAAiB,CAAC,CAAC;EAC5CH,WAAW,CAACS,UAAU,GAAG,IAAI,CAAC,CAAE;EAChCT,WAAW,CAACU,cAAc,GAAG,IAAI,CAAC,CAAE;EACpCV,WAAW,CAACW,IAAI,GAAG,OAAO,CAAC,CAAE;;EAE7B;EACAX,WAAW,CAACY,QAAQ,GAAIC,KAAK,IAAK;IAC9B,IAAIC,iBAAiB,GAAG,EAAE;;IAE1B;IACA,KAAK,IAAIC,CAAC,GAAGF,KAAK,CAACG,WAAW,EAAED,CAAC,GAAGF,KAAK,CAACI,OAAO,CAACC,MAAM,EAAEH,CAAC,EAAE,EAAE;MAC3D,MAAMI,MAAM,GAAGN,KAAK,CAACI,OAAO,CAACF,CAAC,CAAC;;MAE/B;MACA,IAAII,MAAM,CAACC,OAAO,EAAE;QAChBrB,eAAe,IAAIoB,MAAM,CAAC,CAAC,CAAC,CAACE,UAAU,CAAC,CAAE;QAC1ChB,OAAO,CAACiB,GAAG,CAAC,0BAA0B,EAAEvB,eAAe,CAAC,CAAC,CAAE;MAC/D,CAAC,MAAM;QACHe,iBAAiB,IAAIK,MAAM,CAAC,CAAC,CAAC,CAACE,UAAU,CAAC,CAAE;MAChD;IACJ;;IAEA;IACAhB,OAAO,CAACiB,GAAG,CAAC,qBAAqB,EAAER,iBAAiB,CAAC;EACzD,CAAC;;EAED;EACAd,WAAW,CAACuB,KAAK,GAAG,MAAM;IACtBlB,OAAO,CAACiB,GAAG,CAAC,6BAA6B,CAAC;;IAE1C;IACA,IAAIvB,eAAe,EAAE;MACjBM,OAAO,CAACiB,GAAG,CAAC,sCAAsC,CAAC;MACnDE,iBAAiB,CAACzB,eAAe,CAAC,CAAC,CAAE;MACrCA,eAAe,GAAG,EAAE,CAAC,CAAE;IAC3B;;IAEA;IACA,IAAID,oBAAoB,EAAE;MACtBO,OAAO,CAACiB,GAAG,CAAC,kCAAkC,CAAC;MAC/CtB,WAAW,CAACyB,KAAK,CAAC,CAAC,CAAC,CAAE;IAC1B;EACJ,CAAC;;EAED;EACAzB,WAAW,CAAC0B,OAAO,GAAIb,KAAK,IAAK;IAC7BR,OAAO,CAACC,KAAK,CAAC,2BAA2B,EAAEO,KAAK,CAACP,KAAK,CAAC;;IAEvD;IACA,IAAIO,KAAK,CAACP,KAAK,KAAK,WAAW,EAAE;MAC7BC,KAAK,CAAC,uCAAuC,CAAC;MAC9CoB,UAAU,CAAC,MAAM;QACbC,sBAAsB,CAAC,CAAC;MAC5B,CAAC,EAAE,IAAI,CAAC,CAAC,CAAE;IACf;;IAEA;IACA,IAAIf,KAAK,CAACP,KAAK,KAAK,eAAe,EAAE;MACjCC,KAAK,CAAC,2CAA2C,CAAC;IACtD;EACJ,CAAC;AACL;;AAEA;AACA,eAAesB,oBAAoBA,CAAA,EAAG;EAClC,IAAI;IACA,MAAMC,MAAM,GAAG,MAAMC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;MAAEC,KAAK,EAAE;IAAK,CAAC,CAAC;IACzE7B,OAAO,CAACiB,GAAG,CAAC,+BAA+B,CAAC;IAC5CQ,MAAM,CAACK,SAAS,CAAC,CAAC,CAACC,OAAO,CAACC,KAAK,IAAIA,KAAK,CAACC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAE;IACpD,OAAO,IAAI;EACf,CAAC,CAAC,OAAOhC,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,+BAA+B,EAAEA,KAAK,CAAC;IACrDC,KAAK,CAAC,uDAAuD,CAAC;IAC9D,OAAO,KAAK;EAChB;AACJ;;AAEA;AACA,OAAO,eAAeqB,sBAAsBA,CAAA,EAAG;EAC3C,MAAMW,iBAAiB,GAAG,MAAMV,oBAAoB,CAAC,CAAC,CAAC,CAAE;EACzD,IAAI,CAACU,iBAAiB,EAAE;EAExB,IAAIzC,oBAAoB,EAAE;IACtBO,OAAO,CAACC,KAAK,CAAC,wCAAwC,CAAC;IACvD;EACJ;EAEAP,eAAe,GAAG,EAAE,CAAC,CAAE;EACvBS,qBAAqB,CAAC,CAAC,CAAC,CAAE;EAC1B,IAAI;IACAR,WAAW,CAACyB,KAAK,CAAC,CAAC,CAAC,CAAE;IACtB3B,oBAAoB,GAAG,IAAI,CAAC,CAAE;IAC9BO,OAAO,CAACiB,GAAG,CAAC,6BAA6B,CAAC;EAC9C,CAAC,CAAC,OAAOhB,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,oCAAoC,EAAEA,KAAK,CAAC;EAC9D;AACJ;;AAEA;AACA,OAAO,SAASkC,qBAAqBA,CAAA,EAAG;EACpC,IAAI,CAAC1C,oBAAoB,EAAE;IACvBO,OAAO,CAACC,KAAK,CAAC,oCAAoC,CAAC;IACnD;EACJ;EACAD,OAAO,CAACiB,GAAG,CAAC,gCAAgC,CAAC;EAC7CtB,WAAW,CAACsC,IAAI,CAAC,CAAC,CAAC,CAAE;EACrBxC,oBAAoB,GAAG,KAAK,CAAC,CAAE;AACnC;;AAEA;AACA,OAAO,MAAM0B,iBAAiB,GAAG,MAAOH,UAAU,IAAK;EACnD,IAAI;IACA;IACA,MAAMoB,QAAQ,GAAG,MAAMC,KAAK,CAAC,qCAAqC,EAAE;MAChEC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACL,cAAc,EAAE;MACpB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;QAAE1B;MAAW,CAAC,CAAC,CAAG;IAC3C,CAAC,CAAC;IAEF,IAAI,CAACoB,QAAQ,CAACO,EAAE,EAAE;MACd,MAAM,IAAIC,KAAK,CAAC,iBAAiBR,QAAQ,CAACS,MAAM,EAAE,CAAC;IACvD;IAEA,MAAM/B,MAAM,GAAG,MAAMsB,QAAQ,CAACU,IAAI,CAAC,CAAC;IACpC9C,OAAO,CAACiB,GAAG,CAAC,kBAAkB,EAAEH,MAAM,CAAC,CAAC,CAAE;EAC9C,CAAC,CAAC,OAAOb,KAAK,EAAE;IACZD,OAAO,CAACC,KAAK,CAAC,sCAAsC,EAAEA,KAAK,CAAC,CAAC,CAAE;EACnE;AACJ,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}